SECTION I

    - Defining Anomaly Detection
        1. Data x(1), x(2), ... .x(n) where all values of x belongs to a d-dimentional real space
        2. Mixture of nominal and anomaly points
        3. Anomaly points are generated by a different generative process than the normal points

    - Three Settings
        1. SUPERVISED - Training data labeled with "nominal" and "anomaly". There will be severe class imbalance if this approach is used.
        2. CLEAN - Training data are all "nominal", test data contaminated with "anomaly" points. Open catagory case.
        3. UNSUPERVISED - Training data consists of mixture of "nominal" and "anomaly" points.


    - Well-Defined ANomaly Distribution Assumption(works with the Supervised setting)

        1. WDAD: The anomalies are drawn from a well-defined probability distribution
            Example - repeated instances of known machine failures.

        2. The WDAD assumption is often risky:
            a. adversarial situations(fraud, insider threats, cyber security)
            b. diverse set of potential causes(novel device failure modes)
            c. users notion of "anomaly" changes with time(eg anomaly == "intresting point")


    - Stratagies for Unsupervised Anomaly Detection

        Let α be the fraction of training points that are anomalies

            CASE 1: α is large(e.g, > 5%):
                1.1 Fit a 2-component mixture model - actually trying to model the anomaly distribution model.
                1.2 The model assumes that the data comes from two distinct distributions: one for normal data and one for anomalies.
                1.3 We model both distributions and identify anomalies based on the second component (anomaly distribution).           
                    a. requires a WDAD assumption
                    b. Mixture of components must be identifiable
                    c. Mixture of componets cannot have large overlap in high desity regions

            CASE 2: α is small(e.g, 1%, 0.1%, 0.01%, 0.001%)
                2.1 Anomaly detection via outlier detection
                    a. does not require WDAD assumption
                    b. will fail if anomalies are not outliers(eg. overlap with nominal density)
                    c. will fail if nominal distribution has heavy tails


SECTION II

    - Benchmarking Study by Andrew Emmott
        1. Most AD(anomaly detection) papers only evaluate on a few datasets.
        2. often proprietary or very easy(e.g, KDD 1999)
        3. Research community needs a large and growing collection of public anomaly benchmarks


SECTION III

    ALGORITHMS
        1. Z-score: (https://www.youtube.com/watch?v=2tuBREK_mgE&t=66s)
            - Standard Normal Distribution 
                a. Mean(µ) - 0
                b. Standard Deviation(σ) - 1
                c. always centered at zero, intervals of increase or decrease by 1
                d. the each value in the horizondal axix is the z-score
                e. a z-score shows how many standard deviation and observation is from the mean(e.g, a z-sdcore of -2 tells me that we are two steps left if the mean)
                f. allows us to calculate how much area is associated with, using a z-score table (aka standard normal table)
                g. Formula:  Z = (x-µ)/σ
                Where:
                    x is the data point.
                    µ is the mean of the dataset.
                    σ is the standard deviation.

            - How it works:
                Normal data points usually have Z-scores close to 0.
                Anomalies have Z-scores that are far from 0, typically greater than 3 or less than -3, indicating that they are much higher or lower than expected.

            - Advantages:
            Simple and easy to implement.

            - Limitations:
            Assumes data follows a normal distribution (Gaussian), so it may not work well with skewed or heavy-tailed distributions.
            

        2. Moving Average-Based Anomaly Detection:
            This method is based on comparing the current value in a data stream to the moving average of previous values to detect anomalies. The moving average smooths out short-term fluctuations and highlights longer-term trends or cycles.      

            - The key idea is:

                1. Compute the moving average of the last few data points   
                (in this case, the last 10 by default).
                2. Compare the current value to this moving average.
                3. If the deviation (difference between the current value and the moving average) is greater than a certain threshold, the current value is considered an anomaly.

            - Key Parameters:
                1. window: The number of previous values to include in the moving average. Default is 10.
                2. threshold: The acceptable amount of deviation before classifying a value as an anomaly. Default is 2.0.

        3. Isolation Forest:
            a. fast
            b. accurate
            c. will work with large dataset
            d. there is no feature scaling
            e. there is no need to calculate distances which protects you from the curse of dimentionality
            f. its also memory friendly to your machine

            Basic Idea: anomalous points take less splits to ISOLATE

                Random Forest:
                    - You have a dataset D, what you do od split the dataset into subsamples[D(i)] and you create n datasets on which you will build a tree.
                    - Subsample with replacement
                    - Now the size of these D(i)'s is the same size as the D
                    (Size D = Size D(i))
                    - once you have datasets, you want build trees resulting in a model h(1) to h(n)
                    - now you are going split right through the till
                    (right until all the points are isolated -- CLEARLY OVERFITTING)
                    - Another feature is that you only split K features and k < d features
                    (d is the total features)
                    - every time you split it could be a different "k"
                    - and then at the end you classifier is the average of all your classifiers.
                
                Isolated Forest Implementation(I forest):
                    - There are tweaks
                    - you are not going to the end, i.e you are not splitting all the way down
                    (reason)
                    - anomalous points are isolated a lot faster, so you can set the limit on how deep you want to go with the tree.
                    - this is help with REDUCING SWAMPING AND MASKING
                    (SWAMPING = False Positives)
                    (MASKING = False Negatives) 
                    - while we have our trees the system is going to calculate the path length for every point in every tree. 
                    - can vary with dataset samples.
                    - at the end you average acorss the trees for every point
                    (AVERAGE THE PATH LENGTH FOR EACH POINT ACROSS ALL TREES)
                    (PATH LENGTH is the number of nodes between the root and the your points)
                    - The average path length will be a measure for a ANOMALY SCORE, the shorter the path length the higher the anomaly score