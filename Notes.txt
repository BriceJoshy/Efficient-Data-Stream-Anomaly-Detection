SECTION I

    Defining Anomaly Detection
        1. Data x(1), x(2), ... .x(n) where all values of x belongs to a d-dimentional real space
        2. Mixture of nominal and anomaly points
        3. Anomaly points are generated by a different generative process than the normal points

    Three Settings
        1. SUPERVISED - Training data labeled with "nominal" and "anomaly". There will be severe class imbalance if this approach is used.
        2. CLEAN - Training data are all "nominal", test data contaminated with "anomaly" points. Open catagory case.
        3. UNSUPERVISED - Training data consists of mixture of "nominal" and "anomaly" points.


    Well-Defined ANomaly Distribution Assumption(works with the Supervised setting)

        1. WDAD: The anomalies are drawn from a well-defined probability distribution
            Example - repeated instances of known machine failures.

        2. The WDAD assumption is often risky:
            a. adversarial situations(fraud, insider threats, cyber security)
            b. diverse set of potential causes(novel device failure modes)
            c. users notion of "anomaly" changes with time(eg anomaly == "intresting point")


    Stratagies for Unsupervised Anomaly Detection

    Let Œ± be the fraction of training points that are anomalies

        CASE 1: Œ± is large(e.g, > 5%):
            1.1 Fit a 2-component mixture model - actually trying to model the anomaly distribution model.
            1.2 The model assumes that the data comes from two distinct distributions: one for normal data and one for anomalies.
            1.3 We model both distributions and identify anomalies based on the second component (anomaly distribution).           
                a. requires a WDAD assumption
                b. Mixture of components must be identifiable
                c. Mixture of componets cannot have large overlap in high desity regions

        CASE 2: Œ± is small(e.g, 1%, 0.1%, 0.01%, 0.001%)
            2.1 Anomaly detection via outlier detection
                a. does not require WDAD assumption
                b. will fail if anomalies are not outliers(eg. overlap with nominal density)
                c. will fail if nominal distribution has heavy tails


SECTION II

    Benchmarking Study by Andrew Emmott
        1. Most AD(anomaly detection) papers only evaluate on a few datasets.
        2. often proprietary or very easy(e.g, KDD 1999)
        3. Research community needs a large and growing collection of public anomaly benchmarks


SECTION III

    STATISTICAL METHODS
        1. Z-score: (https://www.youtube.com/watch?v=2tuBREK_mgE&t=66s)
            Standard Normal Distribution 
                a. Mean(¬µ) - 0
                b. Standard Deviation(œÉ) - 1
                c. always centered at zero, intervals of increase or decrease by 1
                d. the each value in the horizondal axix is the z-score
                e. a z-score shows how many standard deviation and observation is from the mean(e.g, a z-sdcore of -2 tells me that we are two steps left if the mean)
                f. allows us to calculate how much area is associated with, using a z-score table (aka standard normal table)
                g. Formula:  Z = (x-¬µ)/œÉ
                    Where:
                    x is the data point.
                    ùúá is the mean of the dataset.
                    ùúé is the standard deviation
            
            How it works:
                a. Normal data points usually have Z-scores close to 0.
                    Anomalies have Z-scores that are far from 0, typically greater than 3 or less than -3, indicating that they are much higher or lower than expected.

            Advantages:
                Simple and easy to implement.
            
            Limitations:
                Assumes data follows a normal distribution (Gaussian), so it may not work well with skewed or heavy-tailed distributions.           

        2. Moving Average-Based Anomaly Detection:
            This method is based on comparing the current value in a data stream to the moving average of previous values to detect anomalies. The moving average smooths out short-term fluctuations and highlights longer-term trends or cycles.      

            The key idea is:

            1. Compute the moving average of the last few data points   
            (in this case, the last 10 by default).
            2. Compare the current value to this moving average.
            3. If the deviation (difference between the current value and the moving average) is greater than a certain threshold, the current value is considered an anomaly.